Name: fcn_v_1_6
Log: ./log/
Read data from: /home/boseon/data/kewp/
file: train_x.pkl, shape: (386209, 22)
file: train_y.pkl, shape: (386209, 54)
file: valid_x.pkl, shape: (21457, 22)
file: valid_y.pkl, shape: (21457, 54)
file: test_x.pkl, shape: (21454, 22)
file: test_y.pkl, shape: (21454, 54)
Normalization: z transform
x mean shape: (22,), y mean shape: (54,)
Data reshape into [b, 1, 12, 22]
train: (386197, 1, 12, 22), (386197, 54)
valid: (21445, 1, 12, 22), (21445, 54)
test: (21442, 1, 12, 22), (21442, 54)
batch size: 32
total number of batch: 12068
Check point location: ./log/fcn_v_1_6/
Optimizer: Adam
Learning rate: 1e-07, w/o decay
Activation func: <function relu at 0x7f366673d7b8>
Tensor("X:0", shape=(?, 1, 12, 22), dtype=float32)
Tensor("MaxPool:0", shape=(?, 1, 3, 512), dtype=float32)
Tensor("MaxPool_1:0", shape=(?, 1, 1, 512), dtype=float32)
Tensor("MaxPool_2:0", shape=(?, 1, 1, 512), dtype=float32)
Tensor("Reshape:0", shape=(?, 54), dtype=float32)
Epoch: 0, iter: 12067, train loss: 2.5676916948214057e-07, valid loss: 1.6045403583575535e-07, test mse: 1.6489681005477905
Epoch: 1, iter: 12067, train loss: 2.4390067210333655e-07, valid loss: 1.400780291760384e-07, test mse: 1.48623526096344
Epoch: 2, iter: 12067, train loss: 2.3413716121467587e-07, valid loss: 1.2603952370682237e-07, test mse: 1.3761622905731201
Epoch: 3, iter: 12067, train loss: 2.2648532649327535e-07, valid loss: 1.1615947670406968e-07, test mse: 1.300744652748108
Epoch: 4, iter: 12067, train loss: 2.202002349349641e-07, valid loss: 1.0901868563450989e-07, test mse: 1.2481505870819092
Epoch: 5, iter: 12067, train loss: 2.1482412648765603e-07, valid loss: 1.0369443970148495e-07, test mse: 1.2106297016143799
Epoch: 6, iter: 12067, train loss: 2.101025131651113e-07, valid loss: 9.95954749782868e-08, test mse: 1.1832033395767212
Epoch: 7, iter: 12067, train loss: 2.0579827264555206e-07, valid loss: 9.63465254244511e-08, test mse: 1.1626936197280884
Epoch: 8, iter: 12067, train loss: 2.0178556781047519e-07, valid loss: 9.37020701030633e-08, test mse: 1.1470590829849243
Epoch: 9, iter: 12067, train loss: 1.9801454698153975e-07, valid loss: 9.149901103455704e-08, test mse: 1.134943962097168
Epoch: 10, iter: 12067, train loss: 1.9444304655280575e-07, valid loss: 8.962626196762358e-08, test mse: 1.125436544418335
Epoch: 11, iter: 12067, train loss: 1.9103151771560078e-07, valid loss: 8.800569872846609e-08, test mse: 1.1179124116897583
Epoch: 12, iter: 12067, train loss: 1.877513255976737e-07, valid loss: 8.658194161625943e-08, test mse: 1.1119251251220703
Epoch: 13, iter: 12067, train loss: 1.8459574846474425e-07, valid loss: 8.53150226021171e-08, test mse: 1.1071683168411255
Epoch: 14, iter: 12067, train loss: 1.8155436976030614e-07, valid loss: 8.417647023861718e-08, test mse: 1.1034095287322998
Epoch: 15, iter: 12067, train loss: 1.7860854484297306e-07, valid loss: 8.314403743270304e-08, test mse: 1.1004791259765625
Epoch: 16, iter: 12067, train loss: 1.7576057587120886e-07, valid loss: 8.220067826414379e-08, test mse: 1.098233699798584
Epoch: 17, iter: 12067, train loss: 1.730036132130408e-07, valid loss: 8.133328321946465e-08, test mse: 1.0965510606765747
Epoch: 18, iter: 12067, train loss: 1.7033487154094473e-07, valid loss: 8.053214628489513e-08, test mse: 1.0953782796859741
Epoch: 19, iter: 12067, train loss: 1.6774440325662e-07, valid loss: 7.978945149034189e-08, test mse: 1.0946344137191772
Epoch: 20, iter: 12067, train loss: 1.652327910051099e-07, valid loss: 7.909792998361809e-08, test mse: 1.0942553281784058
Epoch: 21, iter: 12067, train loss: 1.6279479098102456e-07, valid loss: 7.845357430369404e-08, test mse: 1.094254970550537
Epoch: 22, iter: 12067, train loss: 1.604223882623046e-07, valid loss: 7.785102695834212e-08, test mse: 1.0945520401000977
Epoch: 23, iter: 12067, train loss: 1.581114617010826e-07, valid loss: 7.728640838422507e-08, test mse: 1.0951013565063477
Epoch: 24, iter: 12067, train loss: 1.5586049073590402e-07, valid loss: 7.675697588638286e-08, test mse: 1.0958806276321411
Epoch: 25, iter: 12067, train loss: 1.5366455841103743e-07, valid loss: 7.625981623959888e-08, test mse: 1.096872329711914
Epoch: 26, iter: 12067, train loss: 1.5153618448948691e-07, valid loss: 7.579270544511019e-08, test mse: 1.0980677604675293
Epoch: 27, iter: 12067, train loss: 1.494634460641464e-07, valid loss: 7.535348345300008e-08, test mse: 1.0994325876235962
Epoch: 28, iter: 12067, train loss: 1.4744561838142545e-07, valid loss: 7.493997600249713e-08, test mse: 1.1009458303451538
Epoch: 29, iter: 12067, train loss: 1.4548037086115073e-07, valid loss: 7.455062700501003e-08, test mse: 1.1026034355163574
Epoch: 30, iter: 12067, train loss: 1.4356498923007166e-07, valid loss: 7.418393721536631e-08, test mse: 1.1043918132781982
Epoch: 31, iter: 12067, train loss: 1.41695792876817e-07, valid loss: 7.38381444875813e-08, test mse: 1.1062949895858765
Epoch: 32, iter: 12067, train loss: 1.3987822455874266e-07, valid loss: 7.351187747417498e-08, test mse: 1.1083017587661743
Epoch: 33, iter: 12067, train loss: 1.3810910104439245e-07, valid loss: 7.320416273159935e-08, test mse: 1.110407829284668
Epoch: 34, iter: 12067, train loss: 1.3639264295761677e-07, valid loss: 7.291374970463949e-08, test mse: 1.1125956773757935
Epoch: 35, iter: 12067, train loss: 1.3472671867020836e-07, valid loss: 7.263990653427754e-08, test mse: 1.1148614883422852
Epoch: 36, iter: 12067, train loss: 1.3310676649780362e-07, valid loss: 7.238100607764864e-08, test mse: 1.1171740293502808
Epoch: 37, iter: 12067, train loss: 1.3153280065125728e-07, valid loss: 7.21364230571453e-08, test mse: 1.1195392608642578
Epoch: 38, iter: 12067, train loss: 1.3000251897210546e-07, valid loss: 7.190527639977518e-08, test mse: 1.121951937675476
Epoch: 39, iter: 12067, train loss: 1.2851525355017657e-07, valid loss: 7.168688398451195e-08, test mse: 1.1244022846221924
Epoch: 40, iter: 12067, train loss: 1.270659595320467e-07, valid loss: 7.148046421434628e-08, test mse: 1.1268748044967651
Epoch: 41, iter: 12067, train loss: 1.2565972440370388e-07, valid loss: 7.128459600380666e-08, test mse: 1.1293421983718872
Epoch: 42, iter: 12067, train loss: 1.2429077855813375e-07, valid loss: 7.109873223498653e-08, test mse: 1.1318200826644897
Epoch: 43, iter: 12067, train loss: 1.229633994626056e-07, valid loss: 7.092264553421046e-08, test mse: 1.1343159675598145
Epoch: 44, iter: 12067, train loss: 1.2167528495865554e-07, valid loss: 7.075578167814456e-08, test mse: 1.1368154287338257
Epoch: 45, iter: 12067, train loss: 1.2042319497140852e-07, valid loss: 7.059726669922384e-08, test mse: 1.139316201210022
Epoch: 46, iter: 12067, train loss: 1.1920318598868107e-07, valid loss: 7.044667427180684e-08, test mse: 1.1418122053146362
Epoch: 47, iter: 12067, train loss: 1.1801030552760494e-07, valid loss: 7.03030949011918e-08, test mse: 1.1442956924438477
Epoch: 48, iter: 12067, train loss: 1.1684716838544773e-07, valid loss: 7.016643621682306e-08, test mse: 1.1467628479003906
Epoch: 49, iter: 12067, train loss: 1.1571307112490103e-07, valid loss: 7.003634294733274e-08, test mse: 1.149212121963501
Loss graph location: ./loss_graphs/fcn_v_1_6.png
Scalable plot location: ./scalable_plots/
FINAL MSE (w/o overfitting): 1.149212121963501
